{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "W1L2_DataAnalysis.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "1JLlJ0lC4gH7",
        "PYuu0aXo3FZj",
        "axOueTgD7izl",
        "zAIRuz5DB9F_",
        "iHUG4mHgKRFi",
        "FVkSZV4_rpZu",
        "YyUO0VHXtKLr",
        "zMv88pRb5vXp",
        "HqFXIGp38HJJ",
        "554RzZF5BZUS",
        "eWUYadIj8uV6",
        "oR8KiuTIB6pv",
        "FUTeZ3MMJxcX"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#**Data Analysis**\n",
        "<font color='grey' size='1.5'> Created by Parisa Hosseinzadeh for *Machine learning for proteins*, Spring 2022. The examples are adapted from [hands-on tutorial of chapter 2](https://colab.research.google.com/github/ageron/handson-ml2/blob/master/02_end_to_end_machine_learning_project.ipynb#scrollTo=yWpx5Wa71o58)of [hands-on machine learning book](https://www.amazon.ca/Hands-Machine-Learning-Scikit-Learn-TensorFlow/dp/1492032646).\n",
        "\n",
        "In this simple activity, we will learn practice of working with data."
      ],
      "metadata": {
        "id": "IN3c13sF17XS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting up\n",
        "\n",
        "Let's add all the necessary modules and libraries. Let's also mount google drive here."
      ],
      "metadata": {
        "id": "1JLlJ0lC4gH7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Python ≥3.5 is required\n",
        "import sys\n",
        "assert sys.version_info >= (3, 5)\n",
        "\n",
        "# Scikit-Learn ≥0.20 is required\n",
        "import sklearn\n",
        "assert sklearn.__version__ >= \"0.20\"\n",
        "\n",
        "# Common imports\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# To plot pretty figures\n",
        "%matplotlib inline\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "mpl.rc('axes', labelsize=14)\n",
        "mpl.rc('xtick', labelsize=12)\n",
        "mpl.rc('ytick', labelsize=12)\n",
        "\n",
        "# Where to save the figures\n",
        "PROJECT_ROOT_DIR = \".\"\n",
        "CHAPTER_ID = \"W1L2\"\n",
        "IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID)\n",
        "os.makedirs(IMAGES_PATH, exist_ok=True)\n",
        "\n",
        "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
        "    path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\n",
        "    print(\"Saving figure\", fig_id)\n",
        "    if tight_layout:\n",
        "        plt.tight_layout()\n",
        "    plt.savefig(path, format=fig_extension, dpi=resolution)"
      ],
      "metadata": {
        "id": "sqcDzAT64mno"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "V2MPEbuL5T8a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mounting google drive\n",
        "google_drive_mount_point = '/content/google_drive'\n",
        "\n",
        "import os, sys, time\n",
        "\n",
        "if 'google.colab' in sys.modules:\n",
        "    from google.colab import drive\n",
        "    drive.mount(google_drive_mount_point)\n",
        "\n",
        "if not os.getenv(\"DEBUG\"):\n",
        "    google_drive = google_drive_mount_point + '/My Drive' "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ZeLkCJ14rJw",
        "outputId": "4cf58ac0-4eca-418d-d1ec-546074b6292c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/google_drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading data\n",
        "\n",
        "We will use *housig.csv* data for this exercise. "
      ],
      "metadata": {
        "id": "PYuu0aXo3FZj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load data as csv\n",
        "# Replace <Folder> with the correct name of the folder for example MyDrive/BioEML\n",
        "housing = pd.read_csv('/content/google_drive/MyDrive/<Folder>/housing.csv')"
      ],
      "metadata": {
        "id": "J1kBe-ot5Lg8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# taking a look at the first few lines of the file\n",
        "housing.head()"
      ],
      "metadata": {
        "id": "Ubh___FV5cRT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Basic analysis/visualization\n",
        "\n",
        "Let's perform some basic analysis and visualization on our dataset."
      ],
      "metadata": {
        "id": "axOueTgD7izl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's take a look at one of the columns\n",
        "housing['longitude'].hist(bins=50, figsize=(20,15))\n",
        "save_fig(\"longitutde\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3pIe_cOI5iGD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# calculating mean, median, standard deviation\n",
        "mean = housing['longitude'].mean()\n",
        "std = housing['longitude'].std()\n",
        "median = housing['longitude'].median()\n",
        "range = housing['longitude'].max() - housing['longitude'].min()\n",
        "print('For the variable longitude,',\n",
        "      '\\n the mean is:', mean,\n",
        "      '\\n the median is:', median,\n",
        "      '\\n the standard deviation is:', std,\n",
        "      '\\n the range is:', range)"
      ],
      "metadata": {
        "id": "AOhwqUWR7fH-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Time to practice\n",
        "\n",
        "Plot the distribution and claculate the values listed above for *latitude*, *population*, and *total_rooms* and submit to your in-class activity."
      ],
      "metadata": {
        "id": "zAIRuz5DB9F_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#empty code cell for you"
      ],
      "metadata": {
        "id": "HHkVi5xbKNmk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Checking everything at once"
      ],
      "metadata": {
        "id": "iHUG4mHgKRFi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Looking at everything at once\n",
        "housing.describe()"
      ],
      "metadata": {
        "id": "f5wWAcKNCPnc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing all plots\n",
        "housing.hist(bins=50, figsize=(20,15))"
      ],
      "metadata": {
        "id": "_8DgthzBCTke"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# To look at the values of a non-numerical column, you can use the following\n",
        "housing[\"ocean_proximity\"].value_counts()"
      ],
      "metadata": {
        "id": "WDHvkBZ9KPVr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cleaning up data\n",
        "\n",
        "Now let's take a look at how to clean up the data and prepare it for use."
      ],
      "metadata": {
        "id": "J89whxLin4WI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dealing with data with missing value\n",
        "\n",
        "Let's check and see what we should do if we have missing values (NaN) in some of our columns. \n",
        "\n",
        "#### Deleteing missing values\n",
        "\n",
        "One option is to drop any rows that have NaN values. This of course will result in loss of data"
      ],
      "metadata": {
        "id": "FVkSZV4_rpZu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# checking the current length\n",
        "len(housing)"
      ],
      "metadata": {
        "id": "c3yMHMUystXG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# removing the na\n",
        "cleaned_housing = housing.dropna(subset=[\"total_bedrooms\"]) "
      ],
      "metadata": {
        "id": "9bLHZwgMoBQT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Exercise time\n",
        "\n",
        "Check what is the size after removing the rows with no value."
      ],
      "metadata": {
        "id": "YyUO0VHXtKLr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# check the new lines\n"
      ],
      "metadata": {
        "id": "6ZFCJJqHssYf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Replacing missing values\n",
        "\n",
        "Another option is to replace the missing values with something else. One of the most common replacements is median of other values.\n",
        "\n",
        "In scikit learn, you can use [imputers](https://scikit-learn.org/stable/modules/impute.html) to perform this claculation. In this test case, we're using *median* as a strategy for replacement."
      ],
      "metadata": {
        "id": "zMv88pRb5vXp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's take a look at rows with missing values\n",
        "sample_incomplete_rows = housing[housing.isnull().any(axis=1)].head()\n",
        "sample_incomplete_rows"
      ],
      "metadata": {
        "id": "_pst7TwjqF_o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's set the imputer\n",
        "from sklearn.impute import SimpleImputer\n",
        "imputer = SimpleImputer(strategy=\"median\")"
      ],
      "metadata": {
        "id": "u9SjZADmrBPS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# we need to remove the categorical variables because \n",
        "# median doesn't work for them\n",
        "housing_num = housing.drop(\"ocean_proximity\", axis=1)"
      ],
      "metadata": {
        "id": "AoFaJOwvrEoa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# fitting\n",
        "imputer.fit(housing_num)"
      ],
      "metadata": {
        "id": "04aElNDBrFVR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# showing the calculated median values\n",
        "imputer.statistics_"
      ],
      "metadata": {
        "id": "4_-CnFQPrJhv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Performing transformation to replace columns\n",
        "X = imputer.transform(housing_num)"
      ],
      "metadata": {
        "id": "On94CJYrrL5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# generating transformed dataframe\n",
        "housing_tr = pd.DataFrame(X, columns=housing_num.columns,\n",
        "                          index=housing.index)"
      ],
      "metadata": {
        "id": "T-K-1WRArOHZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# let's look at the rows that had missing values\n",
        "housing_tr.loc[sample_incomplete_rows.index.values]"
      ],
      "metadata": {
        "id": "190S3d-IrQBN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# seeing if the rest of data is intact\n",
        "housing_tr.head()"
      ],
      "metadata": {
        "id": "Ufr5nwDJ9L-2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Practice time\n",
        "\n",
        "Repeat the same process with \"most_frequent\" strategy. What is the number you get this time? "
      ],
      "metadata": {
        "id": "HqFXIGp38HJJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# for your code"
      ],
      "metadata": {
        "id": "OebZ3F5q8PYY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Scaling\n",
        "\n",
        "As mentioned in the class, it's often more efficient to scale all the data to make sure they have similar ranges and variations.\n",
        "\n",
        "This often happens after other steps (cleaning, etc) are done.\n",
        "\n",
        "Today, we will use scikit's [StandardScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) to acheive this purpose."
      ],
      "metadata": {
        "id": "yTsPbCpi_P91"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# we need to remove the categorical variables here\n",
        "housing_num = housing.drop(\"ocean_proximity\", axis=1)"
      ],
      "metadata": {
        "id": "jsqbaTYZABqM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# let's take a look at the first rows before scaling\n",
        "housing_num.head()"
      ],
      "metadata": {
        "id": "ABFEILXQAN83"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# fitting data\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(housing_num)"
      ],
      "metadata": {
        "id": "JxQgAD_XAULx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# transforming\n",
        "X = scaler.transform(housing_num)\n",
        "housing_sc = pd.DataFrame(X, columns=housing_num.columns,\n",
        "                          index=housing.index)"
      ],
      "metadata": {
        "id": "ohEKNcG5AjRs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Time to practice\n",
        "\n",
        "Take a look at the first rows of the new dataframe. Compare the new distributions (histograms) to the old one."
      ],
      "metadata": {
        "id": "554RzZF5BZUS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# your code"
      ],
      "metadata": {
        "id": "caKkMtN4AtHY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Handling text/categorical data\n",
        "\n",
        "Now let's take a look at how we can manage text data or categorical data. Let's take a look at one of these data."
      ],
      "metadata": {
        "id": "eWUYadIj8uV6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "housing_cat = housing[[\"ocean_proximity\"]]\n",
        "housing_cat.value_counts()"
      ],
      "metadata": {
        "id": "SxyAOkR283tl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[OrdinalEncoder](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OrdinalEncoder.html) in scikit learn takes in a list and give numbers based on each unique object in the list, changing text/category to numbers."
      ],
      "metadata": {
        "id": "whqLCnAI9oHe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "\n",
        "ordinal_encoder = OrdinalEncoder()\n",
        "housing_cat_encoded = ordinal_encoder.fit_transform(housing_cat)\n",
        "housing_cat_encoded[:10]"
      ],
      "metadata": {
        "id": "VAhH8MGF89RG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# let's look at the categories it found\n",
        "ordinal_encoder.categories_"
      ],
      "metadata": {
        "id": "xcWocvqK96us"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As mentioned in class, there are some challenges with using numbers as categories, as the relation between numbers can imply some relation between data that does not exist. To avoid that, we can one-hot encode categories using scikit's [OneHotEncoder](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html). "
      ],
      "metadata": {
        "id": "D9YE9wxf-Ad9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "cat_encoder = OneHotEncoder()\n",
        "housing_cat_1hot = cat_encoder.fit_transform(housing_cat)\n",
        "# by default, this returns a sparse array\n",
        "# why do you think?\n",
        "housing_cat_1hot"
      ],
      "metadata": {
        "id": "DypKrsew9-yR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# let's take a look at the full array\n",
        "housing_cat_1hot.toarray()"
      ],
      "metadata": {
        "id": "CEtf-cyk-3Mc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# let's see if we found categories correctly\n",
        "cat_encoder.categories_"
      ],
      "metadata": {
        "id": "NfrT4w6j_BJV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pipelines *(optional)* \n",
        "\n",
        "It is often advized to generate a [pipeline](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html) that performs all your transformation and cleaning for you. This way, you can easily run this code consistently everytime you want to use it. \n",
        "\n",
        "Let's take a look at how this is done for this case."
      ],
      "metadata": {
        "id": "oR8KiuTIB6pv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# preparing numerical data\n",
        "num_pipeline = Pipeline([\n",
        "        ('imputer', SimpleImputer(strategy=\"median\")),\n",
        "        ('std_scaler', StandardScaler()),\n",
        "    ])"
      ],
      "metadata": {
        "id": "B4Nx4ZSACUxn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.compose import ColumnTransformer\n",
        "\n",
        "#preparing categorical data\n",
        "num_attribs = list(housing_num)\n",
        "cat_attribs = [\"ocean_proximity\"]\n",
        "\n",
        "full_pipeline = ColumnTransformer([\n",
        "        (\"num\", num_pipeline, num_attribs),\n",
        "        (\"cat\", OneHotEncoder(), cat_attribs),\n",
        "    ])\n",
        "\n",
        "# preparing data\n",
        "housing_prepared = full_pipeline.fit_transform(housing)"
      ],
      "metadata": {
        "id": "eaazWY9ACqTN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# let's look at the final prepared dataframe\n",
        "housing_prepared.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F9vhYog4C9YT",
        "outputId": "0c99f0f4-e1e6-4e6d-8a78-57f061e60d19"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(20640, 14)"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating a test set\n",
        "\n",
        "In this segment, we will practice generating a train/test split."
      ],
      "metadata": {
        "id": "DtFZQUvHIgmD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# we can use scikit for splitting test and train\n",
        "# Let's see how it looks\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_set, test_set = train_test_split(\n",
        "    housing, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "lebW7a6RIqnW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Time to practice\n",
        "\n",
        "How much is the size of test and training set?\n",
        "You can use the function **len**. len(X) where X is a list shows the number of members in the list, aka size."
      ],
      "metadata": {
        "id": "FUTeZ3MMJxcX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Empty code cell for you"
      ],
      "metadata": {
        "id": "_R9snP4nKJfP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What's a good split\n",
        "\n",
        "Let's take a look at one of the variables to see if we had a good split."
      ],
      "metadata": {
        "id": "n4iaY4mNKVTE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "housing[\"median_income\"].hist()"
      ],
      "metadata": {
        "id": "1F3WkkO-ItCL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's say we want to create income categories that are \n",
        "# based on median income. From the histogram above, we chose these cut-offs\n",
        "# to get an even distribution of data among categories\n",
        "housing[\"income_cat\"] = pd.cut(housing[\"median_income\"],\n",
        "                               bins=[0., 1.5, 3.0, 4.5, 6., np.inf],\n",
        "                               labels=[1, 2, 3, 4, 5])"
      ],
      "metadata": {
        "id": "8J71ASAsJZLz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "housing[\"income_cat\"].hist()"
      ],
      "metadata": {
        "id": "OObC6D4aJbr-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Stratified split\n",
        "\n",
        "Let's take a look at another type of splitting here. We use sklearn's [StratifiedShuffleSplit](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedShuffleSplit.html) function which preserves the percentage of samples in each class."
      ],
      "metadata": {
        "id": "0Ct7fWA9LWT8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# splitting using scikit learn\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "\n",
        "split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
        "\n",
        "# it's splitting the IDs and then we get locations based on IDs.\n",
        "for train_index, test_index in split.split(housing, housing[\"income_cat\"]):\n",
        "    strat_train_set = housing.loc[train_index]\n",
        "    strat_test_set = housing.loc[test_index]"
      ],
      "metadata": {
        "id": "tL1kH_0OLbZt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# comparing the two models:\n",
        "# this functions takes in the the number of data in income_cat\n",
        "# normalized by total data\n",
        "def income_cat_proportions(data):\n",
        "    return data[\"income_cat\"].value_counts() / len(data)\n",
        "\n",
        "\n",
        "# calculates the proportion of data in random test set compared to the \n",
        "# overall test set for each category\n",
        "compare_props = pd.DataFrame({\n",
        "    \"Overall\": income_cat_proportions(housing),\n",
        "    \"Stratified\": income_cat_proportions(strat_test_set),\n",
        "    \"Random\": income_cat_proportions(test_set),\n",
        "}).sort_index()\n",
        "compare_props[\"Rand. %error\"] = 100 * compare_props[\"Random\"] / compare_props[\"Overall\"] - 100\n",
        "compare_props[\"Strat. %error\"] = 100 * compare_props[\"Stratified\"] / compare_props[\"Overall\"] - 100"
      ],
      "metadata": {
        "id": "xwXT0ntRLp8N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Time to practice\n",
        "\n",
        "Print out the *compare props* data frame to see which one of the two methods better represent the actual data distribution. Note that the values show the number of members in that category normalized by the total number."
      ],
      "metadata": {
        "id": "3OgcdWIqLv4_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# for you to run"
      ],
      "metadata": {
        "id": "PchR82m2MQpT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}