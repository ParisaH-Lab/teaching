{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "W8L1_VAE.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#**Variational Autoencoder**\n",
        "<font color='grey' size='1.5'> Created by Kevin Harnden for *Machine learning for proteins*, Spring 2022. \n",
        "This notebook is adapted from [Fran√ßois Chollet](https://colab.research.google.com/github/keras-team/keras-io/blob/master/examples/generative/ipynb/vae.ipynb)\n",
        "\n",
        "In today's in-class activity, we will be building a VAE to generate digits 0-9 using the MNIST dataset."
      ],
      "metadata": {
        "id": "WdOY-ODJEYUV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Step 1. Setup"
      ],
      "metadata": {
        "id": "aNIdYf0JHkXB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1lIfVayfET_j"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Step 2. Create a sampling layer\n",
        "\n"
      ],
      "metadata": {
        "id": "uXwqFNJhIUE6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Sampling(layers.Layer):\n",
        "    \"\"\"Uses (z_mean, z_log_var) to sample z, the vector encoding a digit.\"\"\"\n",
        "\n",
        "    def call(self, inputs):\n",
        "        z_mean, z_log_var = inputs\n",
        "        batch = tf.shape(z_mean)[0]\n",
        "        dim = tf.shape(z_mean)[1]\n",
        "        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
        "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon"
      ],
      "metadata": {
        "id": "JGmge2UaIPy8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Step 3. Build the encoder\n",
        "\n",
        "Create an encoder with two convolutional layers, followed by a fully connected layer, and then two separate fully connected layers for the mean and the variance. Finally, the latent layers should be sampled using the Sampling class defined above.\n",
        "\n",
        "The encoder inputs should have dimensions of 28x28x1. The convolutional layers should have 32 and 64 filters, respectively, kernal sizes of 3, strides of 2, \"same\" padding, and use the ReLU activation function. The first fully connected layer should have 16 units and use the ReLU activation funciton. The latent layers should have 2 units each and be named \"z_mean\" and \"z_log_var\".\n",
        "\n",
        "###Q1. Encoder\n",
        "\n",
        "This encoder was designed for the MNIST dataset. For more complex datasets, such as images of faces or protein features, what about the encoder would need to be changed?"
      ],
      "metadata": {
        "id": "ROgg5NiVJW1X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code here\n",
        "\n",
        "latent_dim = None\n",
        "\n",
        "encoder_inputs = None\n",
        "x = None\n",
        "x = None\n",
        "x = None\n",
        "x = None\n",
        "z_mean = None\n",
        "z_log_var = None\n",
        "z = None\n",
        "\n",
        "encoder = keras.Model(encoder_inputs, [z_mean, z_log_var, z], name=\"encoder\")\n",
        "encoder.summary()"
      ],
      "metadata": {
        "id": "KtcnhIaqJbSH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown Sample solution\n",
        "\n",
        "latent_dim = 2\n",
        "\n",
        "encoder_inputs = keras.Input(shape=(28, 28, 1))\n",
        "x = layers.Conv2D(32, 3, activation=\"relu\", strides=2, padding=\"same\")(encoder_inputs)\n",
        "x = layers.Conv2D(64, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
        "x = layers.Flatten()(x)\n",
        "x = layers.Dense(16, activation=\"relu\")(x)\n",
        "z_mean = layers.Dense(latent_dim, name=\"z_mean\")(x)\n",
        "z_log_var = layers.Dense(latent_dim, name=\"z_log_var\")(x)\n",
        "z = Sampling()([z_mean, z_log_var])\n",
        "\n",
        "encoder = keras.Model(encoder_inputs, [z_mean, z_log_var, z], name=\"encoder\")\n",
        "encoder.summary()"
      ],
      "metadata": {
        "id": "-kSZIEnwdID6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Step 4. Build the decoder\n",
        "\n",
        "Create an decoder with a fully connected layer followed by three transposed convolutional layers.\n",
        "\n",
        "The decoder inputs should have the same dimensions as the latent space. The fully connected layer should have dimensions of 7x7x64 and use the ReLU activation funciton. The first two convolutional layers should have 64 and 32 filters, respectively, kernal sizes of 3, strides of 2, \"same\" padding, and use the ReLU activation function. The final convolutional layer should have 1 filter, kernal size of 3, \"same\" padding, and use the sigmoid activation function.\n",
        "\n",
        "###Q2. Decoder\n",
        "\n",
        "What do you notice about the types of layers and their dimensions of the decoder compared to the encoder?"
      ],
      "metadata": {
        "id": "fF9BPER3Jb7h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code here\n",
        "\n",
        "latent_inputs = None\n",
        "x = None\n",
        "x = None\n",
        "x = None\n",
        "x = None\n",
        "decoder_outputs = None\n",
        "\n",
        "decoder = keras.Model(latent_inputs, decoder_outputs, name=\"decoder\")\n",
        "decoder.summary()"
      ],
      "metadata": {
        "id": "78Iy64QOJdwd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown Sample solution\n",
        "\n",
        "latent_inputs = keras.Input(shape=(latent_dim,))\n",
        "x = layers.Dense(7 * 7 * 64, activation=\"relu\")(latent_inputs)\n",
        "x = layers.Reshape((7, 7, 64))(x)\n",
        "x = layers.Conv2DTranspose(64, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
        "x = layers.Conv2DTranspose(32, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
        "decoder_outputs = layers.Conv2DTranspose(1, 3, activation=\"sigmoid\", padding=\"same\")(x)\n",
        "\n",
        "decoder = keras.Model(latent_inputs, decoder_outputs, name=\"decoder\")\n",
        "decoder.summary()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "h2r7xNy1dJFM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Step 5. Define the VAE as a `Model` with a custom `train_step`"
      ],
      "metadata": {
        "id": "iDB9BWutJeKR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class VAE(keras.Model):\n",
        "    def __init__(self, encoder, decoder, **kwargs):\n",
        "        super(VAE, self).__init__(**kwargs)\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.total_loss_tracker = keras.metrics.Mean(name=\"total_loss\")\n",
        "        self.reconstruction_loss_tracker = keras.metrics.Mean(\n",
        "            name=\"reconstruction_loss\"\n",
        "        )\n",
        "        self.kl_loss_tracker = keras.metrics.Mean(name=\"kl_loss\")\n",
        "\n",
        "    @property\n",
        "    def metrics(self):\n",
        "        return [\n",
        "            self.total_loss_tracker,\n",
        "            self.reconstruction_loss_tracker,\n",
        "            self.kl_loss_tracker,\n",
        "        ]\n",
        "\n",
        "    def train_step(self, data):\n",
        "        with tf.GradientTape() as tape:\n",
        "            z_mean, z_log_var, z = self.encoder(data)\n",
        "            reconstruction = self.decoder(z)\n",
        "            reconstruction_loss = tf.reduce_mean(\n",
        "                tf.reduce_sum(\n",
        "                    keras.losses.binary_crossentropy(data, reconstruction), axis=(1, 2)\n",
        "                )\n",
        "            )\n",
        "            kl_loss = -0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n",
        "            kl_loss = tf.reduce_mean(tf.reduce_sum(kl_loss, axis=1))\n",
        "            total_loss = reconstruction_loss + kl_loss\n",
        "        grads = tape.gradient(total_loss, self.trainable_weights)\n",
        "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
        "        self.total_loss_tracker.update_state(total_loss)\n",
        "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
        "        self.kl_loss_tracker.update_state(kl_loss)\n",
        "        return {\n",
        "            \"loss\": self.total_loss_tracker.result(),\n",
        "            \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n",
        "            \"kl_loss\": self.kl_loss_tracker.result(),\n",
        "        }"
      ],
      "metadata": {
        "id": "CYqIPUW-JkZi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Q3. Loss function\n",
        "\n",
        "\n",
        "*   What variables are used to calculate the reconstruction loss?\n",
        "*   What variables are used to calculate the KL divergence loss?\n",
        "*   How will these two losses affect the training?\n",
        "\n"
      ],
      "metadata": {
        "id": "IoTnzok19YmT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Step 6. Train the VAE"
      ],
      "metadata": {
        "id": "dO7hiIqfJoh-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(x_train, _), (x_test, _) = keras.datasets.mnist.load_data()\n",
        "mnist_digits = np.concatenate([x_train, x_test], axis=0)\n",
        "mnist_digits = np.expand_dims(mnist_digits, -1).astype(\"float32\") / 255\n",
        "\n",
        "vae = VAE(encoder, decoder)\n",
        "vae.compile(optimizer=keras.optimizers.Adam())\n",
        "vae.fit(mnist_digits, epochs=10, batch_size=128)"
      ],
      "metadata": {
        "id": "0hPvRAHQJrA3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Step 7. Display a grid of sampled digits\n"
      ],
      "metadata": {
        "id": "80Q5X7MxJwkb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def plot_latent_space(vae, n=30, figsize=15):\n",
        "    # display a n*n 2D manifold of digits\n",
        "    digit_size = 28\n",
        "    scale = 1.0\n",
        "    figure = np.zeros((digit_size * n, digit_size * n))\n",
        "    # linearly spaced coordinates corresponding to the 2D plot\n",
        "    # of digit classes in the latent space\n",
        "    grid_x = np.linspace(-scale, scale, n)\n",
        "    grid_y = np.linspace(-scale, scale, n)[::-1]\n",
        "\n",
        "    for i, yi in enumerate(grid_y):\n",
        "        for j, xi in enumerate(grid_x):\n",
        "            z_sample = np.array([[xi, yi]])\n",
        "            x_decoded = vae.decoder.predict(z_sample)\n",
        "            digit = x_decoded[0].reshape(digit_size, digit_size)\n",
        "            figure[\n",
        "                i * digit_size : (i + 1) * digit_size,\n",
        "                j * digit_size : (j + 1) * digit_size,\n",
        "            ] = digit\n",
        "\n",
        "    plt.figure(figsize=(figsize, figsize))\n",
        "    start_range = digit_size // 2\n",
        "    end_range = n * digit_size + start_range\n",
        "    pixel_range = np.arange(start_range, end_range, digit_size)\n",
        "    sample_range_x = np.round(grid_x, 1)\n",
        "    sample_range_y = np.round(grid_y, 1)\n",
        "    plt.xticks(pixel_range, sample_range_x)\n",
        "    plt.yticks(pixel_range, sample_range_y)\n",
        "    plt.xlabel(\"z[0]\")\n",
        "    plt.ylabel(\"z[1]\")\n",
        "    plt.imshow(figure, cmap=\"Greys_r\")\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "plot_latent_space(vae)"
      ],
      "metadata": {
        "id": "UPZvDBF4Jzq2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Q4. Latent space visualization\n",
        "\n",
        "Are all of the numbers present in the latent space visualization? Do you see interpolations between different numbers?"
      ],
      "metadata": {
        "id": "b4_ubKeRKbUx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Step 8. Display how the latent space clusters different digit classes\n"
      ],
      "metadata": {
        "id": "X5NofqXBJ0cQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_label_clusters(vae, data, labels):\n",
        "    # display a 2D plot of the digit classes in the latent space\n",
        "    z_mean, _, _ = vae.encoder.predict(data)\n",
        "    plt.figure(figsize=(12, 10))\n",
        "    plt.scatter(z_mean[:, 0], z_mean[:, 1], c=labels)\n",
        "    plt.colorbar()\n",
        "    plt.xlabel(\"z[0]\")\n",
        "    plt.ylabel(\"z[1]\")\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "(x_train, y_train), _ = keras.datasets.mnist.load_data()\n",
        "x_train = np.expand_dims(x_train, -1).astype(\"float32\") / 255\n",
        "\n",
        "plot_label_clusters(vae, x_train, y_train)"
      ],
      "metadata": {
        "id": "t21JqoE8J6PC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Q5. Latent space plot\n",
        "\n",
        "Based on the plot above, is the latent space regular or irregular? What is your reasoning?"
      ],
      "metadata": {
        "id": "PuN34LCqaYHW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Q6. Sampling the latent space\n",
        "\n",
        "Write a function that manually samples the latent space from user defined values and displays the output as an image (similar to the function defined above). What happends when values far outside of the distribution from the plot above?"
      ],
      "metadata": {
        "id": "_MBBLp_z_kif"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code here"
      ],
      "metadata": {
        "id": "qpc-8R54dtTo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown Sample solution\n",
        "\n",
        "def sample_latent_space(vae, z0, z1, figsize=5):\n",
        "    digit_size = 28\n",
        "    digit = np.zeros((digit_size, digit_size))\n",
        "\n",
        "    x_decoded = vae.decoder.predict([[z0,z1]])\n",
        "    digit = x_decoded[0].reshape(digit_size, digit_size)\n",
        "\n",
        "    plt.figure(figsize=(figsize, figsize))\n",
        "    plt.imshow(digit, cmap=\"Greys_r\")\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "sample_latent_space(vae,0,0)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "PggG-48x_jOO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}