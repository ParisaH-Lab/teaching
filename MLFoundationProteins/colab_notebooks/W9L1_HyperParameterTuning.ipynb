{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "W9L1_HyperParameterTuning.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#**Automated hyperparameter tuning**\n",
        "<font color='grey' size='1.5'> Created by Parisa Hosseinzadeh for *Machine learning for proteins*, Spring 2022. \n",
        "\n",
        "Today, we will work on two different examples of hyperparameter tunining in learning. In the first example, we go through hyperparameter tunining for a random forest classifier. In the second part, we will perform tuning on a deep learning model.\n",
        "\n",
        "We're training both models on MNIST data.\n",
        "\n",
        "A bit about MNIST dataset from [MNIST wikipedia page](https://en.wikipedia.org/wiki/MNIST_database):\n",
        "\n",
        "The MNIST database (Modified National Institute of Standards and Technology database) is a large database of handwritten digits that is commonly used for training various image processing systems. \n",
        "\n",
        "<img src=\"https://upload.wikimedia.org/wikipedia/commons/2/27/MnistExamples.png?format=250w\" >"
      ],
      "metadata": {
        "id": "QfvtVuKXdB60"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Random forest on MNIST data"
      ],
      "metadata": {
        "id": "wi6xyu_EEf8n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading required modules"
      ],
      "metadata": {
        "id": "TOJjnWiOd4vo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "from sklearn.model_selection import RandomizedSearchCV"
      ],
      "metadata": {
        "id": "9iZIv51rPfAr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading and preparing the dataset"
      ],
      "metadata": {
        "id": "cpFCjiX2d-Io"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fetching MNIST Dataset\n",
        "mnist = fetch_openml('mnist_784', version=1)\n",
        "\n",
        "# Get the data and target\n",
        "X, y = mnist[\"data\"], mnist[\"target\"]\n",
        "\n",
        "# Split the train and test set\n",
        "X_train, X_test, y_train, y_test = X[:60000], X[60000:], y[:60000], y[60000:]"
      ],
      "metadata": {
        "id": "UzyL5ON-Qwzb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Building model and testing"
      ],
      "metadata": {
        "id": "qTCE8c2lE9lx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Training on the existing dataset\n",
        "rf_clf = RandomForestClassifier(\n",
        "                      random_state= 42, # to make sure numbers are reproducible\n",
        "                      bootstrap=True, # To reduce correlation \n",
        "                      max_depth = 1, # number of features\n",
        "                      n_estimators = 20, # number of trees\n",
        "                      )\n",
        "\n",
        "# fitting\n",
        "rf_clf.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "GoTNFmYJoe34"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluation"
      ],
      "metadata": {
        "id": "Qwv19VPiFSaN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluating the model\n",
        "y_pred = rf_clf.predict(X_test)\n",
        "score = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy score after training on existing dataset\", score)"
      ],
      "metadata": {
        "id": "-65ulJ_2FQWZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hyperparameter tuning\n",
        "\n",
        "As you can see, the accuracy isn't ... great. So, we need to change a bunch of parameters to make the random forest better."
      ],
      "metadata": {
        "id": "Vj8DvrhxFV0d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Q1. Possible parameters\n",
        "\n",
        "Check [Scikit's random forest classifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html). What parameters are available for you to tune?"
      ],
      "metadata": {
        "id": "cq98YrmLFn7g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Checking current parameters\n",
        "\n",
        "Let's take a look and see which parameters are currently being used and what's their value."
      ],
      "metadata": {
        "id": "PLXDFRn7F1f7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pprint import pprint\n",
        "\n",
        "# Look at parameters used by our current forest\n",
        "print('Parameters currently in use:\\n')\n",
        "pprint(rf_clf.get_params())"
      ],
      "metadata": {
        "id": "d-u3kPsQFqnJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Choosing parameters to tune\n",
        "\n",
        "As you can see, there are a lot of parameters to tune. It is not efficient to tune all of them, so we usually just focus on a subset."
      ],
      "metadata": {
        "id": "V_a0TW5JGGy-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Q2. Parameter to tune\n",
        "\n",
        "What are the top 5 most important parameters that you'll pick to tune?"
      ],
      "metadata": {
        "id": "BNcoaGXtGQa3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Building a grid and tuning\n",
        "\n",
        "We usually pick the following parameters for a random forest: (parameters and hyperameter tuning adapted from [TDS - Will Koehersen](https://towardsdatascience.com/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74))\n",
        "\n",
        "- `n_estimators` = number of trees in the foreset\n",
        "- `max_features` = max number of features considered for splitting a node\n",
        "- `max_depth` = max number of levels in each decision tree\n",
        "- `min_samples_split` = min number of data points placed in a node before the node is split\n",
        "- `min_samples_leaf` = min number of data points allowed in a leaf node\n",
        "- `bootstrap` = method for sampling data points (with or without replacement)\n",
        "\n",
        "The first step is to give each parameter some values we would like to search."
      ],
      "metadata": {
        "id": "dD55LnFSGZkY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Note: Fewer options are written to save time for this activity\n",
        "# Number of trees in random forest\n",
        "n_estimators = [int(x) for x in np.linspace(start = 20, stop = 200, num = 4)]\n",
        "# Number of features to consider at every split\n",
        "max_features = ['auto', 'sqrt']\n",
        "# Maximum number of levels in tree\n",
        "max_depth = [int(x) for x in np.linspace(10, 90, num = 5)]\n",
        "max_depth.append(None)\n",
        "# Minimum number of samples required to split a node\n",
        "min_samples_split = [2, 5, 10]\n",
        "# Minimum number of samples required at each leaf node\n",
        "min_samples_leaf = [1, 2, 4]\n",
        "# Method of selecting samples for training each tree\n",
        "bootstrap = [True, False]"
      ],
      "metadata": {
        "id": "ZKFOCbiQGUqv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then, we would like to build a random grid that contains all these values."
      ],
      "metadata": {
        "id": "vtTglDHXG8nN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the random grid\n",
        "random_grid = {'n_estimators': n_estimators,\n",
        "               'max_features': max_features,\n",
        "               'max_depth': max_depth,\n",
        "               'min_samples_split': min_samples_split,\n",
        "               'min_samples_leaf': min_samples_leaf,\n",
        "               'bootstrap': bootstrap}\n",
        "pprint(random_grid)"
      ],
      "metadata": {
        "id": "wdpKFqyeG8Kr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Q3. Total combinations\n",
        "\n",
        "How many total combinations exists for the grid you set?"
      ],
      "metadata": {
        "id": "nwFr9exoHaxJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see, we have to search a lot of combinations if we want to search everything. To avoid this, we perform a random search. The benefit of a random search is that we are not trying every combination, but selecting at random to sample a wide range of values.\n",
        "\n",
        "Note that there is also the option to try every combination. It's called *Grid search* and you can learn more about it at [GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html). It is highly recommended to use smaller combinations if you want to try that."
      ],
      "metadata": {
        "id": "dO_JAqMXHNH7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the random grid to search for best hyperparameters\n",
        "# First create the base model to tune\n",
        "rf = RandomForestClassifier()\n",
        "# Random search of parameters, using 3 fold cross validation, \n",
        "# search across 5 different combinations, and use all available cores\n",
        "# We chose 5 to save class time. Usually it tests 100 or so.\n",
        "rf_random = RandomizedSearchCV(estimator = rf, \n",
        "                               param_distributions = random_grid, \n",
        "                               n_iter = 5, \n",
        "                               cv = 2, \n",
        "                               verbose=1, \n",
        "                               random_state=42, \n",
        "                               n_jobs = -1)\n",
        "# Fit the random search model\n",
        "rf_random.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "yLZq1gmTHqnR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Q4. Time \n",
        "\n",
        "How long did it take to run through your random search?"
      ],
      "metadata": {
        "id": "YJ0Ox1tyIAHZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Getting the best model\n",
        "\n",
        "Now let's get the best parameter and see how much our accuracy changed."
      ],
      "metadata": {
        "id": "BC2NqE9cIG7x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rf_random.best_params_\n",
        "{'bootstrap': True,\n",
        " 'max_depth': 70,\n",
        " 'max_features': 'auto',\n",
        " 'min_samples_leaf': 4,\n",
        " 'min_samples_split': 10,\n",
        " 'n_estimators': 400}"
      ],
      "metadata": {
        "id": "_C5qGpHmIFoj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Q5. New accuracy\n",
        "\n",
        "Rebuild your random forest with these parameters and find the accuracy on test set. What is your conclusion?"
      ],
      "metadata": {
        "id": "LZJlbTT2INem"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# your code here"
      ],
      "metadata": {
        "id": "zzZidBWHIVtW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown Sample code\n",
        "\n",
        "# Training on the existing dataset\n",
        "rf_clf = RandomForestClassifier(\n",
        "                      bootstrap=True,\n",
        "                      max_depth=70,\n",
        "                      max_features='auto',\n",
        "                      min_samples_leaf=4,\n",
        "                      min_samples_split=10,\n",
        "                      n_estimators=400\n",
        ")\n",
        "\n",
        "# fitting\n",
        "rf_clf.fit(X_train, y_train)\n",
        "\n",
        "# Evaluating the model\n",
        "y_pred = rf_clf.predict(X_test)\n",
        "score = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy score after training on existing dataset\", score)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "d1ofmZJVIXOK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Convolutional neural nets\n",
        "\n",
        "We can also perform hypermapater tuning on deep learning models. Here, we will try it on a CNN trained on MNIST data."
      ],
      "metadata": {
        "id": "3DUSjXjbIWoD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading necessary modules"
      ],
      "metadata": {
        "id": "115cEVmyJ5WA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install keras-tuner --upgrade"
      ],
      "metadata": {
        "id": "fQvqwoy4Qxcd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten, Conv2D\n",
        "from tensorflow.keras.losses import sparse_categorical_crossentropy\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from kerastuner.tuners import RandomSearch"
      ],
      "metadata": {
        "id": "QnXiCu7hJV_8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading in and preparing data"
      ],
      "metadata": {
        "id": "qL7gfARDJ8eX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "img_width, img_height, img_num_channels = 28, 28, 1\n",
        "\n",
        "# Load MNIST data\n",
        "(input_train, target_train), (input_test, target_test) = mnist.load_data()\n",
        "\n",
        "# Reshape data\n",
        "input_train = input_train.reshape(input_train.shape[0], img_width, img_height, 1)\n",
        "input_test = input_test.reshape(input_test.shape[0], img_width, img_height, 1)\n",
        "\n",
        "# Determine shape of the data\n",
        "input_shape = (img_width, img_height, img_num_channels)\n",
        "\n",
        "# Parse numbers as floats\n",
        "input_train = input_train.astype('float32')\n",
        "input_test = input_test.astype('float32')\n",
        "\n",
        "# Scale data\n",
        "input_train = input_train / 255\n",
        "input_test = input_test / 255"
      ],
      "metadata": {
        "id": "e5ZK8WabKBNT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Building models\n",
        "\n",
        "We first set the main parameters of our model. "
      ],
      "metadata": {
        "id": "UBd8-633KCuJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model configuration\n",
        "batch_size = 100\n",
        "loss_function = sparse_categorical_crossentropy\n",
        "no_classes = 10\n",
        "# changed epochs from 25 to 5 for speeding up\n",
        "no_epochs = 5\n",
        "validation_split = 0.2\n",
        "verbosity = 1"
      ],
      "metadata": {
        "id": "Tw0dicT1KRI9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then we build our CNN model. As you can see, it is a simple CNN with 2 CNN layers and two dense layer. But one of the dense layer has a choice of number of dimentions. We also have a choice of learning rate."
      ],
      "metadata": {
        "id": "txbArQJrLRBS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# MODEL BUILDING FUNCTION\n",
        "def build_model(hp):\n",
        "  # Create the model\n",
        "  model = Sequential()\n",
        "  model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=input_shape))\n",
        "  model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
        "  model.add(Flatten())\n",
        "  hp_units = hp.Int('units', min_value=64, max_value=218, step=32)\n",
        "  model.add(Dense(hp_units, activation='relu'))\n",
        "  model.add(Dense(no_classes, activation='softmax'))\n",
        "\n",
        "  # Display a model summary\n",
        "  model.summary()\n",
        "\n",
        "  # Compile the model\n",
        "  model.compile(loss=loss_function,\n",
        "                optimizer=Adam(\n",
        "                  hp.Choice('learning_rate',\n",
        "                            values=[1e-2, 1e-3, 1e-4])),# making learning rate tunable by setting values\n",
        "                metrics=['accuracy'])\n",
        "  \n",
        "  # Return the model\n",
        "  return model"
      ],
      "metadata": {
        "id": "qSzQO-pBJWmP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tuning\n",
        "\n",
        "We will also be building a random search for CNN to perform tuning. It's done very similar to the random forest. This time, we're going over only 5 trials."
      ],
      "metadata": {
        "id": "iLY_D9SbKlbj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform tuning\n",
        "# I changed max_trials from 5 --> 3 to save time\n",
        "# I changed execution from 3 --> 1 to save time\n",
        "tuner = RandomSearch(\n",
        "    build_model,\n",
        "    objective='val_accuracy',\n",
        "    max_trials=3,\n",
        "    executions_per_trial=1,\n",
        "    directory='tuning_dir',\n",
        "    project_name='machinecurve_example')"
      ],
      "metadata": {
        "id": "5xoq5eHBJcqp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display search space summary\n",
        "tuner.search_space_summary()\n",
        "\n",
        "# Perform random search\n",
        "# changed epochs from 5 to 3 to save time\n",
        "tuner.search(input_train, target_train,\n",
        "             epochs=3,\n",
        "             validation_split=validation_split)"
      ],
      "metadata": {
        "id": "ZLMYgAS4Jhvn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Q6. Time and setting\n",
        "\n",
        "How long does this take? What other parameters we could change?"
      ],
      "metadata": {
        "id": "dnVIgMQoRNfh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Getting the best model and evaluating"
      ],
      "metadata": {
        "id": "h3oMlZAcMnTY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get best model\n",
        "models = tuner.get_best_models(num_models=1)\n",
        "best_model = models[0]\n",
        "\n",
        "# Fit data to model\n",
        "history = best_model.fit(input_train, target_train,\n",
        "            batch_size=batch_size,\n",
        "            epochs=no_epochs,\n",
        "            verbose=verbosity,\n",
        "            validation_split=validation_split)\n"
      ],
      "metadata": {
        "id": "pGkbDylhJiP_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate generalization metrics\n",
        "score = best_model.evaluate(input_test, target_test, verbose=0)\n",
        "print(f'Test loss: {score[0]} / Test accuracy: {score[1]}')"
      ],
      "metadata": {
        "id": "DDvdYyUBnvqZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Q7. Accuracy\n",
        "\n",
        "What is the best accuracy? What's your conclusion?"
      ],
      "metadata": {
        "id": "SiO9gNTpRWdk"
      }
    }
  ]
}